{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scenedetect opencv-python moviepy transformers sentence-transformers git+https://github.com/openai/whisper.git\n",
        "!apt install ffmpeg -y"
      ],
      "metadata": {
        "id": "6XIem-EelE-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import whisper\n",
        "import numpy as np\n",
        "import moviepy.editor as mp\n",
        "import base64\n",
        "from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datetime import timedelta\n",
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "id": "YPK7Ckr52Tya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scene Detection\n",
        "def detect_scenes(video_path, threshold=30.0):\n",
        "    video_manager = VideoManager([video_path])\n",
        "    scene_manager = SceneManager()\n",
        "    scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
        "    video_manager.set_downscale_factor()\n",
        "    video_manager.start()\n",
        "    scene_manager.detect_scenes(frame_source=video_manager)\n",
        "    return scene_manager.get_scene_list()"
      ],
      "metadata": {
        "id": "Dj0h5u102XgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyframe Extraction (optional: use histogram variance for best frame)\n",
        "def extract_keyframes(video_path, scenes, output_dir=\"keyframes\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    keyframes = []\n",
        "    for idx, (start, end) in enumerate(scenes):\n",
        "        cap.set(cv2.CAP_PROP_POS_MSEC, start.get_seconds() * 1000)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            filename = f\"{output_dir}/scene_{idx+1}.jpg\"\n",
        "            cv2.imwrite(filename, frame)\n",
        "            keyframes.append(filename)\n",
        "    cap.release()\n",
        "    return keyframes"
      ],
      "metadata": {
        "id": "fyMeeiFh2bTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcription using Whisper\n",
        "def transcribe_audio(video_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(video_path)\n",
        "    return result[\"text\"], result[\"segments\"]"
      ],
      "metadata": {
        "id": "KDvlvcpU2e_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Align transcript to scenes using timestamps\n",
        "from whisper.utils import format_timestamp\n",
        "\n",
        "def split_transcript_by_scenes_with_timestamps(segments, scenes):\n",
        "    chunks = []\n",
        "    for start, end in scenes:\n",
        "        start_time = start.get_seconds()\n",
        "        end_time = end.get_seconds()\n",
        "        text = \" \".join([seg['text'] for seg in segments if seg['start'] >= start_time and seg['end'] <= end_time])\n",
        "        chunks.append(text.strip())\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "BDwLa45y2iiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU-safe summarizer with auto length control\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)"
      ],
      "metadata": {
        "id": "bYlaxKUC2l2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_chunks(text_chunks, min_ratio=0.25, max_ratio=0.5):\n",
        "    summaries = []\n",
        "    for idx, chunk in enumerate(text_chunks):\n",
        "        word_count = len(chunk.split())\n",
        "        if word_count < 10:\n",
        "            summaries.append(\"Text too short to summarize.\")\n",
        "            continue\n",
        "\n",
        "        if word_count > 500:\n",
        "            words = chunk.split()\n",
        "            sub_summaries = []\n",
        "            for i in range(0, len(words), 250):\n",
        "                sub_chunk = \" \".join(words[i:i+250])\n",
        "                try:\n",
        "                    summary = summarizer(sub_chunk, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
        "                except Exception as e:\n",
        "                    summary = \"(partial) Could not summarize.\"\n",
        "                sub_summaries.append(summary)\n",
        "            summaries.append(\" \".join(sub_summaries))\n",
        "        else:\n",
        "            try:\n",
        "                max_len = min(80, max(20, int(word_count * max_ratio)))\n",
        "                min_len = min(40, max(10, int(word_count * min_ratio)))\n",
        "                summary = summarizer(chunk, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
        "            except Exception as e:\n",
        "                summary = \"Could not summarize.\"\n",
        "            summaries.append(summary)\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "r1yH0v6h2qh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge similar summaries using cosine similarity\n",
        "def merge_similar_summaries(summaries, threshold=0.9):\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(summaries, convert_to_tensor=True)\n",
        "    merged = []\n",
        "    used = set()\n",
        "    for i in range(len(summaries)):\n",
        "        if i in used:\n",
        "            continue\n",
        "        group = [summaries[i]]\n",
        "        used.add(i)\n",
        "        for j in range(i + 1, len(summaries)):\n",
        "            if j not in used:\n",
        "                sim = util.pytorch_cos_sim(embeddings[i], embeddings[j])\n",
        "                if sim >= threshold:\n",
        "                    used.add(j)\n",
        "        merged.append(group[0])\n",
        "    return merged"
      ],
      "metadata": {
        "id": "qWIEH5qG2uSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate HTML Output with Base64 images\n",
        "def generate_html_output(scenes, keyframes, summaries, save_path=\"summary.html\"):\n",
        "    html = \"<h2>Video Scene Summarization</h2><br>\"\n",
        "    for i, (scene, img_path, summary) in enumerate(zip(scenes, keyframes, summaries)):\n",
        "        start_time = str(timedelta(seconds=int(scene[0].get_seconds())))\n",
        "        end_time = str(timedelta(seconds=int(scene[1].get_seconds())))\n",
        "\n",
        "        with open(img_path, \"rb\") as image_file:\n",
        "            encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "        img_tag = f'<img src=\"data:image/jpeg;base64,{encoded}\" width=\"300\"/>'\n",
        "\n",
        "        html += f\"\"\"\n",
        "        <div style='border:1px solid #ccc; padding:10px; margin:10px'>\n",
        "            <h4>Scene {i+1} | {start_time} - {end_time}</h4>\n",
        "            {img_tag}<br>\n",
        "            <b>Summary:</b> {summary}\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html)\n",
        "    display(HTML(html))\n",
        "    print(f\"Summary HTML saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "v7kTUYqO2xxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Pipeline\n",
        "def run_pipeline(video_path):\n",
        "    print(\"Detecting scenes...\")\n",
        "    scenes = detect_scenes(video_path)\n",
        "\n",
        "    if not scenes:\n",
        "        print(\"No scenes detected. Using full video as one chunk.\")\n",
        "        from scenedetect.frame_timecode import FrameTimecode\n",
        "        clip = mp.VideoFileClip(video_path)\n",
        "        duration = int(clip.duration)\n",
        "        scenes = [(FrameTimecode(0), FrameTimecode(duration))]\n",
        "        clip.close()\n",
        "\n",
        "    print(\"Extracting keyframes...\")\n",
        "    keyframes = extract_keyframes(video_path, scenes)\n",
        "\n",
        "    print(\"Transcribing audio with Whisper...\")\n",
        "    transcript, segments = transcribe_audio(video_path)\n",
        "\n",
        "    print(\"Splitting transcript by scene using timestamps...\")\n",
        "    text_chunks = split_transcript_by_scenes_with_timestamps(segments, scenes)\n",
        "\n",
        "    print(\"Summarizing scenes...\")\n",
        "    summaries = summarize_chunks(text_chunks)\n",
        "\n",
        "    print(\"Merging redundant summaries...\")\n",
        "    final_summaries = merge_similar_summaries(summaries)\n",
        "\n",
        "    print(\"Generating HTML summary report...\")\n",
        "    generate_html_output(scenes, keyframes, final_summaries)\n",
        "    print(\"Pipeline completed successfully.\")"
      ],
      "metadata": {
        "id": "pW1FS1U-21gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_and_run():\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "    run_pipeline(video_path)\n",
        "    files.download(\"summary.html\")"
      ],
      "metadata": {
        "id": "vDqrz7JF27M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_and_run()"
      ],
      "metadata": {
        "id": "mB3ZNb1tuVTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_zCGa_Sv6Uq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}